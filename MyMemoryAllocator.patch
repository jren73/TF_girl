diff -Naur /home/cc/tensorflow_clean/tensorflow/tensorflow/core/platform/posix/env.cc tensorflow/core/platform/posix/env.cc
--- /home/cc/tensorflow_clean/tensorflow/tensorflow/core/platform/posix/env.cc	2019-07-24 21:57:15.285169254 +0000
+++ tensorflow/core/platform/posix/env.cc	2019-06-28 19:23:52.963218829 +0000
@@ -25,6 +25,10 @@
 #include <time.h>
 #include <unistd.h>
 
+#ifdef __FreeBSD__
+#include <pthread_np.h>
+#endif
+
 #include <thread>
 #include <vector>
 
@@ -110,10 +114,7 @@
     pthread_threadid_np(nullptr, &tid64);
     return static_cast<int32>(tid64);
 #elif defined(__FreeBSD__)
-    // Has to be casted to long first, else this error appears:
-    // static_cast from 'pthread_t' (aka 'pthread *') to 'int32' (aka 'int')
-    // is not allowed
-    return static_cast<int32>(static_cast<int64>(pthread_self()));
+    return pthread_getthreadid_np();
 #else
     return static_cast<int32>(pthread_self());
 #endif
@@ -133,7 +134,12 @@
     return false;
 #else
     char buf[100];
+#ifdef __FreeBSD__
+    int res = 0;
+    pthread_get_name_np(pthread_self(), buf, static_cast<size_t>(100));
+#else
     int res = pthread_getname_np(pthread_self(), buf, static_cast<size_t>(100));
+#endif
     if (res != 0) {
       return false;
     }
diff -Naur /home/cc/tensorflow_clean/tensorflow/tensorflow/core/platform/posix/port.cc tensorflow/core/platform/posix/port.cc
--- /home/cc/tensorflow_clean/tensorflow/tensorflow/core/platform/posix/port.cc	2019-07-24 21:57:15.285169254 +0000
+++ tensorflow/core/platform/posix/port.cc	2019-07-24 21:49:10.477715908 +0000
@@ -21,6 +21,7 @@
 #include "tensorflow/core/platform/numa.h"
 #include "tensorflow/core/platform/snappy.h"
 #include "tensorflow/core/platform/types.h"
+#include "tensorflow/core/platform/posix/port.h"
 
 #if defined(__linux__) && !defined(__ANDROID__)
 #include <sched.h>
@@ -33,10 +34,13 @@
 #include <cpuid.h>
 #endif
 
+#include <iostream>
 #include <stdio.h>
 #include <stdlib.h>
 #include <string.h>
 #include <unistd.h>
+#include <sys/mman.h>
+#include <map>
 #ifdef TF_USE_SNAPPY
 #include "snappy.h"
 #endif
@@ -49,10 +53,20 @@
 #include "hwloc.h"  // TF:hwloc
 #endif
 
+#define KB(x)   ((size_t) (x) << 10)
+#define MB(x)   ((size_t) (x) << 20)
+//const size_t pagesize = sysconf(_SC_PAGESIZE);
+//const size_t linesize = sizeof(void*);
+//using namespace std;
+
 namespace tensorflow {
 namespace port {
 
-void InitMain(const char* usage, int* argc, char*** argv) {}
+MyMemoryAllocator MMA;
+
+void InitMain(const char* usage, int* argc, char*** argv) {
+//	MMA.SetValue(1);
+}
 
 string Hostname() {
   char hostname[1024];
@@ -82,6 +96,16 @@
 
 int MaxParallelism() { return NumSchedulableCPUs(); }
 
+int MaxParallelism(int numa_node) {
+  if (numa_node != port::kNUMANoAffinity) {
+    // Assume that CPUs are equally distributed over available NUMA nodes.
+    // This may not be true, but there isn't currently a better way of
+    // determining the number of CPUs specific to the requested node.
+    return NumSchedulableCPUs() / port::NUMANumNodes();
+  }
+  return NumSchedulableCPUs();
+}
+
 int NumTotalCPUs() {
   int count = absl::base_internal::NumCPUs();
   return (count <= 0) ? kUnknownCPU : count;
@@ -211,31 +235,59 @@
 }
 
 void* AlignedMalloc(size_t size, int minimum_alignment) {
+  //jie:
+  //if( minimum_alignment > 64)
+  //  printf("size = %d, minimum_alignment = %u\n", size, minimum_alignment);
 #if defined(__ANDROID__)
   return memalign(minimum_alignment, size);
 #else  // !defined(__ANDROID__)
-  void* ptr = nullptr;
+  //void* ptr = nullptr;
   // posix_memalign requires that the requested alignment be at least
   // sizeof(void*). In this case, fall back on malloc which should return
   // memory aligned to at least the size of a pointer.
-  const int required_alignment = sizeof(void*);
-  if (minimum_alignment < required_alignment) return Malloc(size);
-  int err = posix_memalign(&ptr, minimum_alignment, size);
+  //const int required_alignment = sizeof(void*);
+  //if (minimum_alignment < required_alignment) return Malloc(size);
+  /*{
+    return  MMA.SmallMalloc();
+  }*/
+  //  return MMA.MMAMalloc(size);
+  //int real_allocate = (minimum_alignment+10)/pagesize;
+  /*if (size/pagesize >= 1)
+  {
+	//printf("use one page\n");
+	ptr = mmap(0, size, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
+	if( ptr != nullptr)
+	{
+		MMA.SetValue(ptr, size);
+		return ptr;
+	}
+}*/
+  /*int err = posix_memalign(&ptr, minimum_alignment, size);
   if (err != 0) {
     return nullptr;
   } else {
     return ptr;
-  }
+  }*/
+  return MMA.MyAllocate(size);
+
 #endif
 }
 
-void AlignedFree(void* aligned_memory) { Free(aligned_memory); }
+void AlignedFree(void* aligned_memory)// { Free(aligned_memory); }
+{MMA.MyFree(aligned_memory);}
 
 void* Malloc(size_t size) { return malloc(size); }
-
+//{
+//  return MMA.MyAllocate(size);
+//}
 void* Realloc(void* ptr, size_t size) { return realloc(ptr, size); }
 
-void Free(void* ptr) { free(ptr); }
+void Free(void* ptr) {
+//	MMA.MyFree(ptr);}
+//if(ptr==NULL)
+//  return;
+//printf("Call Free\n");
+free(ptr); }
 
 void* NUMAMalloc(int node, size_t size, int minimum_alignment) {
 #ifdef TENSORFLOW_USE_NUMA
@@ -260,7 +312,7 @@
     return;
   }
 #endif  // TENSORFLOW_USE_NUMA
-  Free(ptr);
+  AlignedFree(ptr);
 }
 
 int NUMAGetMemAffinity(const void* addr) {
diff -Naur /home/cc/tensorflow_clean/tensorflow/tensorflow/core/platform/posix/port.h tensorflow/core/platform/posix/port.h
--- /home/cc/tensorflow_clean/tensorflow/tensorflow/core/platform/posix/port.h	1970-01-01 00:00:00.000000000 +0000
+++ tensorflow/core/platform/posix/port.h	2019-07-24 19:00:52.901036154 +0000
@@ -0,0 +1,250 @@
+/*#include <unistd.h>
+#include <signal.h>
+#include <stdio.h>
+#include <malloc.h>
+#include <stdlib.h>
+#include <errno.h>
+#include <sys/mman.h>
+
+class Pool_c
+{ // Basic type define
+   typedef unsigned int uint;
+   typedef unsigned char uchar;
+
+   uint m_numOfBlocks; // Num of blocks
+   uint m_sizeOfEachBlock; // Size of each block
+   uint m_numFreeBlocks; // Num of remaining blocks
+   uint m_numInitialized; // Num of initialized blocks
+   uchar* m_memStart; // Beginning of memory pool
+   uchar* m_next; // Num of next free block
+   //const size_t pagesize = sysconf(_SC_PAGESIZE);
+
+ public:
+    Pool_c()
+    {
+        m_numOfBlocks = 0;
+        m_sizeOfEachBlock = 0;
+        m_numFreeBlocks = 0;
+        m_numInitialized = 0;
+        m_memStart = NULL;
+        m_next = 0;
+    }
+    ~Pool_c() { DestroyPool(); }
+    void CreatePool(size_t sizeOfEachBlock,uint numOfBlocks)
+    {
+        m_numOfBlocks = numOfBlocks;
+        m_sizeOfEachBlock = sizeOfEachBlock;
+        //m_memStart = new uchar[ m_sizeOfEachBlock * m_numOfBlocks ];
+        void* ptr;
+        int err = posix_memalign(&ptr, sysconf(_SC_PAGESIZE),  m_sizeOfEachBlock * m_numOfBlocks);
+        if (err != 0) {
+          ptr = NULL;
+          m_numFreeBlocks = 0;
+        }
+        else{
+          m_numFreeBlocks = numOfBlocks;
+        }
+        m_memStart = (uchar*)ptr;
+        m_next = m_memStart;
+    }
+    void DestroyPool()
+    {
+        //delete[] m_memStart;
+        free((void*)m_memStart);
+        m_memStart = NULL;
+    }
+    uchar* AddrFromIndex(uint i) const
+    {
+        return m_memStart + ( i * m_sizeOfEachBlock );
+    }
+    uint IndexFromAddr(const uchar* p) const
+    {
+        return (((uint)(p - m_memStart)) / m_sizeOfEachBlock);
+    }
+    void* Allocate()
+    {
+        if (m_numInitialized < m_numOfBlocks )
+        {
+            uint* p = (uint*)AddrFromIndex( m_numInitialized );
+            *p = m_numInitialized + 1;
+            m_numInitialized++;
+        }
+        void* ret = NULL;
+        if ( m_numFreeBlocks > 0 )
+        {
+            ret = (void*)m_next;
+            --m_numFreeBlocks;
+            if (m_numFreeBlocks!=0)
+            {
+                m_next = AddrFromIndex( *((uint*)m_next) );
+            }
+            else
+            {
+                m_next = NULL;
+            }
+        }
+        return ret;
+    }
+    void DeAllocate(void* p)
+     {
+         if (m_next != NULL)
+         {
+             (*(uint*)p) = IndexFromAddr( m_next );
+             m_next = (uchar*)p;
+         }
+         else
+         {
+             *((uint*)p) = m_numOfBlocks;
+             m_next = (uchar*)p;
+         }
+         ++m_numFreeBlocks;
+     }
+}; // End pool class
+*/
+#include <sys/mman.h>
+#include <pjlib.h>
+#include <map>
+#include <list>
+#include <mutex>
+#include <unistd.h>
+#include <stdlib.h>
+#include <malloc.h>
+#include "third_party/eigen3/unsupported/Eigen/CXX11/Tensor"
+#define KB(x)   ((size_t) (x) << 10)
+#define MB(x)   ((size_t) (x) << 20)
+
+/*
+class Pool_c{
+private:
+    pj_caching_pool cp;
+    pj_status_t status;
+    int reserved_size;
+    pj_pool_t *pool;
+    static void my_perror(const char *title, pj_status_t status)
+    {
+        PJ_PERROR(1,(THIS_FILE, status, title));
+    }
+
+public:
+    Pool_c(int _reserved_size = 1024)
+    {
+        reserved_size = _reserved_size;
+        //init pjlib
+        status = pj_init();
+        if (status != PJ_SUCCESS) {
+            my_perror("Error initializing PJLIB", status);
+            return;
+        }
+
+        //create pool factory
+        pj_caching_pool_init(&cp, NULL, reserved_size*MB(1) );
+        pool = pj_pool_create(cp.pfactory, // the factory
+                              "pool1", // pool's name
+                              reserved_size*MB(1), // initial size
+                              MB(1), // increment size
+                              NULL); // use default callback.
+        if (pool == NULL) {
+          my_perror("Error creating pool", PJ_ENOMEM);
+          return;
+        }
+    }
+
+    void* allocate(size_t size)
+    {
+        return pj_pool_alloc(pool, size);
+    }
+
+    ~Pool_c(){
+        // Done with silly demo, must free pool to release all memory.
+        pj_pool_release(pool);
+        // Done with demos, destroy caching pool before exiting app.
+        pj_caching_pool_destroy(&cp);
+    }
+
+
+};
+*/
+
+
+class MyMemoryAllocator{
+private:
+    std::map<void*, int> m;          //record<ptr, size> pair for quick free
+    std::map<int, std::list<void*> > l;    //record<size, list<ptr>> for quick allocation
+    size_t pool_size;           //pool size in MB
+    std::mutex l_alloc;
+    std::mutex l_free;
+    //Pool_c *p;
+    const size_t pagesize = sysconf(_SC_PAGESIZE);
+    const size_t linesize = sizeof(void*);
+
+
+public:
+    MyMemoryAllocator(size_t _pool_size = 1){
+        pool_size = _pool_size;
+        //p = Pool_c(pool_size);
+    }
+
+    void* MyAllocate(size_t size)
+    {
+       if(size <= linesize)
+        {
+            size = linesize;
+        }
+        else{
+          size = (size/pagesize + 1)*pagesize;
+        }
+        //int page = size/pagesize + 1;
+
+        void* ptr;
+        if (!l[size].empty())
+        {
+            l_alloc.lock();
+            ptr = l[size].front();
+            m[ptr] = size;
+            l[size].pop_front();
+            l_alloc.unlock();
+            return ptr;
+        }
+        else{
+            //ptr = p.allocate(size);
+            int err = posix_memalign(&ptr, EIGEN_MAX_ALIGN_BYTES, size);
+            //ptr = mmap(0, page, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
+            if (err != 0) {
+              return nullptr;
+            } else {
+              l_alloc.lock();
+              m[ptr] = size;
+              l_alloc.unlock();
+              return ptr;
+            }
+            //ptr = memalign(EIGEN_MAX_ALIGN_BYTES, page*pagesize);
+            //if(ptr!=NULL)
+            //  m[ptr] = page;
+        }
+        //return ptr;
+    }
+
+    void MyFree(void* ptr)
+    {
+
+        if(ptr == NULL)
+          return;
+        int size = m[ptr];
+        if(size)
+        {
+            l_alloc.lock();
+            l[size].push_back(ptr);
+            //free(ptr);
+            m[ptr] = 0;
+            l_alloc.unlock();
+        }
+        /*else{
+          free(ptr);
+        }*/
+    }
+
+    ~MyMemoryAllocator(){
+
+    }
+
+};
